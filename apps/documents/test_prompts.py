"""
Tests for Medical Document Processing Prompts Module

Tests the MediExtract prompt system including specialized prompts,
confidence scoring, and integration with DocumentAnalyzer.

Like running a full diagnostic on the truck before taking it
on a long haul - gotta make sure everything works right.
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
import json
import logging
import os
import django
from typing import Dict, List, Any

# Configure Django settings for testing
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'meddocparser.settings.development')
django.setup()

from apps.documents.prompts import (
    MedicalPrompts, 
    ProgressivePromptStrategy, 
    ConfidenceScoring,
    ChunkInfo, 
    ContextTag
)


# Test fixtures and sample data
SAMPLE_MEDICAL_DOCUMENT = """
EMERGENCY DEPARTMENT VISIT REPORT

Patient: Smith, John
DOB: 01/15/1980
MRN: 12345678
Gender: Male

Chief Complaint: Chest pain

History of Present Illness:
43-year-old male presents with acute onset chest pain, radiating to left arm.
Pain started 2 hours ago, rated 8/10.

Vital Signs:
BP: 140/90 mmHg
HR: 110 bpm
Temp: 98.6Â°F
O2 Sat: 98% on room air

Assessment and Plan:
1. Acute chest pain - rule out MI
2. EKG ordered
3. Cardiac enzymes
4. Aspirin 81mg given
5. Monitor in ED

Disposition: Admitted to cardiology for further evaluation
"""

SAMPLE_EXTRACTED_FIELDS = [
    {"label": "Patient Name", "value": "Smith, John", "confidence": 0.95},
    {"label": "Date of Birth", "value": "01/15/1980", "confidence": 0.90},
    {"label": "Medical Record Number", "value": "12345678", "confidence": 0.95},
    {"label": "Chief Complaint", "value": "Chest pain", "confidence": 0.88},
    {"label": "Vital Signs", "value": "BP: 140/90 mmHg, HR: 110 bpm", "confidence": 0.85},
    {"label": "Medications", "value": "Aspirin 81mg", "confidence": 0.80},
]


class TestMedicalPrompts(unittest.TestCase):
    """Test the MedicalPrompts class for proper prompt generation."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.logger = logging.getLogger(__name__)
    
    def test_get_primary_extraction_prompt(self):
        """Test getting the primary MediExtract prompt."""
        prompt = MedicalPrompts.get_extraction_prompt()
        
        # Should return the primary MediExtract prompt
        self.assertIn("MediExtract", prompt)
        self.assertIn("medical documents", prompt)
        self.assertIn("JSON object", prompt)
        self.assertIn("confidence", prompt)
        
        # Should include medical field examples
        self.assertIn("patientName", prompt)
        self.assertIn("dateOfBirth", prompt)
        self.assertIn("diagnoses", prompt)
    
    def test_get_emergency_department_prompt(self):
        """Test ED-specific prompt generation."""
        prompt = MedicalPrompts.get_extraction_prompt(document_type='ed')
        
        # Should be ED-specific
        self.assertIn("Emergency Department", prompt)
        self.assertIn("chiefComplaint", prompt)
        self.assertIn("triageLevel", prompt)
        self.assertIn("emergencyProcedures", prompt)
        self.assertIn("disposition", prompt)
    
    def test_get_surgical_prompt(self):
        """Test surgical document prompt generation."""
        prompt = MedicalPrompts.get_extraction_prompt(document_type='surgical')
        
        # Should be surgical-specific
        self.assertIn("surgical documentation", prompt)
        self.assertIn("preOpDiagnosis", prompt)
        self.assertIn("postOpDiagnosis", prompt)
        self.assertIn("surgeon", prompt)
        self.assertIn("anesthesia", prompt)
    
    def test_get_lab_prompt(self):
        """Test laboratory document prompt generation."""
        prompt = MedicalPrompts.get_extraction_prompt(document_type='lab')
        
        # Should be lab-specific
        self.assertIn("laboratory documentation", prompt)
        self.assertIn("labResults", prompt)
        self.assertIn("collectionDate", prompt)
        self.assertIn("abnormalFlags", prompt)
        self.assertIn("referenceRanges", prompt)
    
    def test_get_fhir_prompt(self):
        """Test FHIR-specific prompt generation."""
        prompt = MedicalPrompts.get_extraction_prompt(fhir_focused=True)
        
        # Should be FHIR-focused
        self.assertIn("FHIR", prompt)
        self.assertIn("Fast Healthcare Interoperability Resources", prompt)
        self.assertIn("Patient", prompt)
        self.assertIn("Condition", prompt)
        self.assertIn("Observation", prompt)
        self.assertIn("MedicationStatement", prompt)
    
    def test_get_chunked_document_prompt(self):
        """Test chunked document prompt generation."""
        chunk_info = ChunkInfo(current=2, total=5)
        prompt = MedicalPrompts.get_extraction_prompt(chunk_info=chunk_info)
        
        # Should indicate chunking
        self.assertIn("part 2 of 5", prompt)
        self.assertIn("portion of a larger", prompt)
        self.assertIn("document section", prompt)
        self.assertIn("merge results", prompt)
    
    def test_prompt_with_context_tags(self):
        """Test prompt enhancement with context tags."""
        context_tags = [
            ContextTag(text="Emergency Department Visit"),
            ContextTag(text="Cardiac evaluation")
        ]
        
        prompt = MedicalPrompts.get_extraction_prompt(context_tags=context_tags)
        
        # Should include context
        self.assertIn("Context:", prompt)
        self.assertIn("Emergency Department Visit", prompt)
        self.assertIn("Cardiac evaluation", prompt)
    
    def test_prompt_with_additional_instructions(self):
        """Test prompt enhancement with additional instructions."""
        additional_instructions = "Focus specifically on cardiac medications and procedures"
        
        prompt = MedicalPrompts.get_extraction_prompt(
            additional_instructions=additional_instructions
        )
        
        # Should include additional instructions
        self.assertIn("Additional instructions:", prompt)
        self.assertIn("cardiac medications", prompt)
    
    def test_get_fallback_prompt(self):
        """Test fallback prompt retrieval."""
        prompt = MedicalPrompts.get_fallback_prompt()
        
        # Should be simplified
        self.assertIn("medical data extraction assistant", prompt)
        self.assertIn("simplified format", prompt)
        self.assertIn("patient_name", prompt)
        self.assertIn("diagnoses", prompt)
        
        # Should be less complex than primary prompt
        self.assertNotIn("patientName", prompt)  # Uses simplified field names
    
    def test_document_type_recognition_from_context(self):
        """Test automatic document type recognition from context."""
        # Test ED recognition
        context_tags = [ContextTag(text="Emergency Department Report")]
        prompt = MedicalPrompts.get_extraction_prompt(context_tags=context_tags)
        # This should trigger ED prompt due to context parsing in DocumentAnalyzer
        
        # Test surgical recognition
        context_tags = [ContextTag(text="Post-surgical Follow-up")]
        prompt = MedicalPrompts.get_extraction_prompt(context_tags=context_tags)
        
        # Test lab recognition
        context_tags = [ContextTag(text="Laboratory Results")]
        prompt = MedicalPrompts.get_extraction_prompt(context_tags=context_tags)
        
        # All should include context
        self.assertIn("Context:", prompt)


class TestProgressivePromptStrategy(unittest.TestCase):
    """Test the progressive prompt strategy for fallback handling."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.mock_client = Mock()
        self.strategy = ProgressivePromptStrategy(self.mock_client)
    
    def test_fallback_strategy_initialization(self):
        """Test strategy initialization with proper prompt sequence."""
        self.assertEqual(len(self.strategy.prompt_sequence), 3)
        
        # Should have primary, fhir, and fallback prompts
        prompt_names = [name for name, _ in self.strategy.prompt_sequence]
        self.assertIn("primary", prompt_names)
        self.assertIn("fhir", prompt_names)
        self.assertIn("fallback", prompt_names)
    
    def test_extract_with_fallbacks_success(self):
        """Test successful extraction with fallback strategy."""
        result = self.strategy.extract_with_fallbacks("test content")
        
        # Should return success and prompt information
        self.assertTrue(result['success'])
        self.assertIn('prompt', result)
        self.assertIn('prompt_type', result)
        self.assertEqual(result['attempt'], 1)
    
    def test_extract_with_context(self):
        """Test extraction with context information."""
        context = "Emergency Department Report"
        result = self.strategy.extract_with_fallbacks("test content", context=context)
        
        # Should include context in prompt
        self.assertTrue(result['success'])
        self.assertIn(context, result['prompt'])


class TestConfidenceScoring(unittest.TestCase):
    """Test the confidence scoring and calibration system."""
    
    def test_calibrate_confidence_scores(self):
        """Test confidence score calibration."""
        fields = SAMPLE_EXTRACTED_FIELDS.copy()
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should have same number of fields
        self.assertEqual(len(calibrated), len(fields))
        
        # Should add confidence metadata
        for field in calibrated:
            self.assertIn('confidence_level', field)
            self.assertIn('requires_review', field)
            self.assertIsInstance(field['requires_review'], bool)
    
    def test_confidence_calibration_patient_name(self):
        """Test patient name confidence calibration."""
        # Good patient name
        fields = [{"label": "Patient Name", "value": "Smith, John", "confidence": 0.8}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should increase confidence for full name
        self.assertGreater(calibrated[0]['confidence'], 0.8)
        
        # Poor patient name
        fields = [{"label": "Patient Name", "value": "J", "confidence": 0.8}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should decrease confidence for incomplete name
        self.assertLessEqual(calibrated[0]['confidence'], 0.3)
    
    def test_confidence_calibration_dates(self):
        """Test date field confidence calibration."""
        # Good date format
        fields = [{"label": "Date of Birth", "value": "01/15/1980", "confidence": 0.7}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should increase confidence for proper date format
        self.assertGreater(calibrated[0]['confidence'], 0.7)
        
        # Poor date format
        fields = [{"label": "Date of Birth", "value": "sometime in 1980", "confidence": 0.7}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should decrease confidence for poor date format (has digits, so may not decrease as much)
        # The current logic keeps confidence if there are any digits
        self.assertLessEqual(calibrated[0]['confidence'], 0.7)  # Should not increase
    
    def test_confidence_calibration_mrn(self):
        """Test medical record number confidence calibration."""
        # Good MRN
        fields = [{"label": "Medical Record Number", "value": "12345678", "confidence": 0.7}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should increase confidence for numeric MRN
        self.assertGreater(calibrated[0]['confidence'], 0.7)
        
        # Poor MRN
        fields = [{"label": "MRN", "value": "AB", "confidence": 0.7}]
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Should decrease confidence for non-numeric MRN
        self.assertLessEqual(calibrated[0]['confidence'], 0.5)
    
    def test_confidence_levels(self):
        """Test confidence level categorization."""
        # Test high confidence
        level = ConfidenceScoring._get_confidence_level(0.9)
        self.assertEqual(level, "high")
        
        # Test medium confidence
        level = ConfidenceScoring._get_confidence_level(0.6)
        self.assertEqual(level, "medium")
        
        # Test low confidence
        level = ConfidenceScoring._get_confidence_level(0.4)
        self.assertEqual(level, "low")
        
        # Test very low confidence
        level = ConfidenceScoring._get_confidence_level(0.2)
        self.assertEqual(level, "very_low")
    
    def test_quality_metrics(self):
        """Test quality metrics generation."""
        fields = SAMPLE_EXTRACTED_FIELDS.copy()
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        metrics = ConfidenceScoring.get_quality_metrics(calibrated)
        
        # Should include all expected metrics
        expected_keys = [
            'total_fields', 'avg_confidence', 'high_confidence_count',
            'requires_review_count', 'quality_score', 'confidence_distribution'
        ]
        
        for key in expected_keys:
            self.assertIn(key, metrics)
        
        # Metrics should be reasonable
        self.assertEqual(metrics['total_fields'], len(fields))
        self.assertGreaterEqual(metrics['avg_confidence'], 0.0)
        self.assertLessEqual(metrics['avg_confidence'], 1.0)
        self.assertGreaterEqual(metrics['quality_score'], 0.0)
        self.assertLessEqual(metrics['quality_score'], 100.0)
    
    def test_quality_metrics_empty_fields(self):
        """Test quality metrics with empty field list."""
        metrics = ConfidenceScoring.get_quality_metrics([])
        
        # Should handle empty gracefully
        self.assertEqual(metrics['total_fields'], 0)
        self.assertEqual(metrics['avg_confidence'], 0.0)
        self.assertEqual(metrics['quality_score'], 0.0)


class TestDocumentAnalyzerIntegration(unittest.TestCase):
    """Test integration of prompt system with DocumentAnalyzer."""
    
    def setUp(self):
        """Set up test fixtures."""
        # Mock the DocumentAnalyzer imports
        self.patcher = patch('apps.documents.services.anthropic')
        self.mock_anthropic = self.patcher.start()
        
        # Create mock analyzer with minimal setup
        with patch('apps.documents.services.DocumentAnalyzer._init_ai_clients'):
            from apps.documents.services import DocumentAnalyzer
            self.analyzer = DocumentAnalyzer(api_key="test_key")
            self.analyzer.logger = Mock()
    
    def tearDown(self):
        """Clean up patches."""
        self.patcher.stop()
    
    def test_prompt_selection_basic(self):
        """Test basic prompt selection without specific context."""
        prompt = self.analyzer._get_medical_extraction_prompt()
        
        # Should return primary MediExtract prompt
        self.assertIn("MediExtract", prompt)
        self.assertIn("JSON object", prompt)
    
    def test_prompt_selection_with_context(self):
        """Test prompt selection with context."""
        # Test ED context
        prompt = self.analyzer._get_medical_extraction_prompt("Emergency Department Report")
        self.assertIn("Context:", prompt)
        
        # Test surgical context
        prompt = self.analyzer._get_medical_extraction_prompt("Surgical Report")
        self.assertIn("Context:", prompt)
    
    def test_prompt_selection_with_chunking(self):
        """Test prompt selection for chunked documents."""
        chunk_info = {
            'current': 2,
            'total': 4,
            'is_first': False,
            'is_last': False
        }
        
        prompt = self.analyzer._get_medical_extraction_prompt(
            context="Medical Report",
            chunk_info=chunk_info
        )
        
        # Should indicate chunking
        self.assertIn("part 2 of 4", prompt)
    
    @patch('apps.documents.services.ResponseParser')
    def test_ai_response_parsing_with_confidence(self, mock_parser_class):
        """Test AI response parsing with confidence calibration."""
        # Setup mock parser
        mock_parser = Mock()
        mock_parser_class.return_value = mock_parser
        mock_parser.extract_structured_data.return_value = SAMPLE_EXTRACTED_FIELDS.copy()
        mock_parser.validate_parsed_fields.return_value = {
            "is_valid": True,
            "field_count": len(SAMPLE_EXTRACTED_FIELDS),
            "avg_confidence": 0.87,
            "issues": []
        }
        
        # Test parsing
        result = self.analyzer._parse_ai_response('{"test": "response"}')
        
        # Should have called parser
        mock_parser.extract_structured_data.assert_called_once()
        mock_parser.validate_parsed_fields.assert_called_once()
        
        # Should return calibrated fields
        self.assertIsInstance(result, list)
        if result:  # If fields were returned
            for field in result:
                self.assertIn('confidence_level', field)
                self.assertIn('requires_review', field)
    
    def test_fallback_extraction_method(self):
        """Test fallback extraction method exists and is callable."""
        # Should have fallback method
        self.assertTrue(hasattr(self.analyzer, '_try_fallback_extraction'))
        self.assertTrue(callable(self.analyzer._try_fallback_extraction))


class TestPromptSystemEndToEnd(unittest.TestCase):
    """End-to-end tests for the complete prompt system."""
    
    def test_prompt_system_components(self):
        """Test that all major prompt system components work together."""
        # Test prompt generation
        prompt = MedicalPrompts.get_extraction_prompt(
            document_type='ed',
            context_tags=[ContextTag(text="Emergency Department")],
            additional_instructions="Focus on cardiac symptoms"
        )
        
        # Should include all enhancements
        self.assertIn("Emergency Department", prompt)
        self.assertIn("Context:", prompt)
        self.assertIn("Additional instructions:", prompt)
        self.assertIn("cardiac symptoms", prompt)
    
    def test_confidence_scoring_pipeline(self):
        """Test complete confidence scoring pipeline."""
        # Start with basic fields
        fields = [
            {"label": "Patient Name", "value": "Smith, John", "confidence": 0.8},
            {"label": "MRN", "value": "123", "confidence": 0.7},  # Short MRN
            {"label": "Date of Birth", "value": "no date found", "confidence": 0.6}  # Poor date
        ]
        
        # Apply calibration
        calibrated = ConfidenceScoring.calibrate_confidence_scores(fields)
        
        # Generate metrics
        metrics = ConfidenceScoring.get_quality_metrics(calibrated)
        
        # Verify end-to-end processing
        self.assertEqual(len(calibrated), 3)
        self.assertIn('total_fields', metrics)
        self.assertEqual(metrics['total_fields'], 3)
        
        # Check that calibration affected scores appropriately
        name_field = next(f for f in calibrated if 'name' in f['label'].lower())
        date_field = next(f for f in calibrated if 'date' in f['label'].lower())
        
        # Name should have increased (good format)
        self.assertGreater(name_field['confidence'], 0.8)
        
        # Date should have decreased (poor format)
        self.assertLessEqual(date_field['confidence'], 0.4)


if __name__ == '__main__':
    # Configure logging for tests
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Run tests
    unittest.main(verbosity=2) 